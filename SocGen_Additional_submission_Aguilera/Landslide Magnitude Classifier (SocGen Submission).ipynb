{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import Packages and Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/quintonaguilera/Desktop/Quinton | Aldo | Luigi/landslide_data.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define Functions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all functions will be explained in great detail, but the build_multi_output_model function will, because of its importance throughout the notebook:\n",
    "\n",
    "The function builds a neural network comprised of essentially 5 parts:\n",
    "\n",
    "1) The input layer\n",
    "\n",
    "2) The first collection of hidden layers\n",
    "\n",
    "3) The first output\n",
    "\n",
    "4) The second collection of hidden layers\n",
    "\n",
    "5) The second output\n",
    "\n",
    "\n",
    "Each non-output layer has a dropout layer following it (dropout rate = 0.2), and all layers have an orthagonal kernel initializer, with the exceptino of the input layer. L1 and L2 regularisation is used for all hidden layers. \n",
    "\n",
    "The function requires being passed the number of layers in the first collection of hidden layers (stage_1_hidden_layers), and the number of neurons in each layer (stage_1_neurons), as well as the same for the second stage of hidden layers. The activation functions for the first stage are rely, and 'leaky relu' for the second stage (this was done to combat the somehat common \"dying relu\" problem). The activation functions for the outputs are sigmoid and softmax, corresponding to the output formats required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_muti_output_model(stage_1_hidden_layers, stage_2_hidden_layers, \n",
    "                            stage_1_neurons, stage_2_neurons):\n",
    "    input_size=19\n",
    "    stage1_counter = 0\n",
    "    stage2_counter = 0\n",
    "    \n",
    "    input_layer = keras.layers.Input(input_size, \n",
    "                                     name=\"input\")\n",
    "\n",
    "    while stage1_counter < (stage_1_hidden_layers):\n",
    "        if stage1_counter == 0:\n",
    "            x = keras.layers.Dense(stage_1_neurons, activation=\"relu\", \n",
    "                                   kernel_initializer=tf.keras.initializers.Orthogonal(),\n",
    "                                   kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)\n",
    "                                  )(input_layer)\n",
    "            x = keras.layers.Dropout(0.2)(x)\n",
    "            stage1_counter += 1\n",
    "        else:\n",
    "            x = keras.layers.Dense(stage_1_neurons, activation=\"relu\",\n",
    "                                  kernel_initializer=tf.keras.initializers.Orthogonal(),\n",
    "                                  kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)\n",
    "                                  )(x)\n",
    "            x = keras.layers.Dropout(0.2)(x)\n",
    "            stage1_counter += 1\n",
    "        \n",
    "    count_output = keras.layers.Dense(1, activation=\"sigmoid\",\n",
    "                                      kernel_initializer=tf.keras.initializers.Orthogonal(),\n",
    "                                      name=\"count_output\")(x)\n",
    "    \n",
    "    while stage2_counter < (stage_2_hidden_layers):\n",
    "        if stage1_counter == 0:\n",
    "            x = keras.layers.Dense(stage_2_neurons, activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n",
    "                                   kernel_initializer=tf.keras.initializers.Orthogonal(),\n",
    "                                   kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(count_output)\n",
    "            x = keras.layers.Dropout(0.2)(x)\n",
    "            stage2_counter += 1\n",
    "        else:\n",
    "            x = keras.layers.Dense(stage_2_neurons, activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "                                  kernel_initializer=tf.keras.initializers.Orthogonal(),\n",
    "                                  kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(x)\n",
    "            x = keras.layers.Dropout(0.2)(x)\n",
    "            stage2_counter += 1\n",
    "    \n",
    "    class_output = keras.layers.Dense(4, activation=\"softmax\",\n",
    "                                      kernel_initializer=tf.keras.initializers.Orthogonal(),\n",
    "                                      name=\"class_output\")(x)\n",
    "\n",
    "\n",
    "    model = keras.Model(inputs=input_layer, outputs=[count_output, class_output]) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte carlo approach to finding optimal hyper parameters\n",
    "\n",
    "def monte_carlo_tuning(x_train, y_train_1, y_train_2, \n",
    "                       first_stage_hl, second_stage_hl,\n",
    "                       first_stage_n, second_stage_n,\n",
    "                       runs=10, n_samples=1):\n",
    "    min_loss = 0.0\n",
    "    min_count_loss = 0.0\n",
    "    min_area_loss = 0.0\n",
    "    max_conf = []\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                          mode='min', verbose=0, patience=0, restore_best_weights=True)\n",
    "\n",
    "    config_cache_loss = {}\n",
    "    config_cache_count = {}\n",
    "    config_cache_area = {}\n",
    "    \n",
    "    for _ in tqdm(range(runs)):\n",
    "        \n",
    "        configuration = generate_configuration(first_stage_hl, second_stage_hl,\n",
    "                                      first_stage_n, second_stage_n)\n",
    "        \n",
    "        if all(configuration) not in config_cache_loss:\n",
    "            loss = 0.0\n",
    "            count_loss = 0.0\n",
    "            area_loss = 0.0\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                model = build_muti_output_model(configuration[0], configuration[1],\n",
    "                                               configuration[2], configuration[3])\n",
    "\n",
    "                model.compile(loss=[\"binary_crossentropy\", \"mse\"],\n",
    "                              optimizer=\"adam\")\n",
    "\n",
    "\n",
    "                history = model.fit(x_train, [y_train_1, y_train_2], \n",
    "                    epochs=5,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(patience=15)],\n",
    "                    verbose=0)\n",
    "                \n",
    "                loss       += history.history['val_loss'][-1]\n",
    "                count_loss += history.history['val_count_output_loss'][-1]\n",
    "                area_loss  += history.history['val_area_output_loss'][-1]\n",
    "            loss        = loss/n_samples\n",
    "            count_loss  = count_loss/n_samples\n",
    "            area_loss   = area_loss/n_samples\n",
    "\n",
    "            config_cache_loss[str(configuration)]  = loss\n",
    "            config_cache_count[str(configuration)] = count_loss\n",
    "            config_cache_area[str(configuration)]  = area_loss\n",
    "\n",
    "    return config_cache_loss, config_cache_count, config_cache_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a datalist and a number of breakpoints, this function finds the break point values such that the\n",
    "# variance in each category is minimised\n",
    "\n",
    "def getJenksBreaks(dataList, numClass):\n",
    "    dataList = list(dataList)\n",
    "    dataList.sort()\n",
    "    mat1 = []\n",
    "    for i in range(0,len(dataList)+1):\n",
    "        temp = []\n",
    "        for j in range(0,numClass+1):\n",
    "            temp.append(0)\n",
    "        mat1.append(temp)\n",
    "    mat2 = []\n",
    "    for i in range(0,len(dataList)+1):\n",
    "        temp = []\n",
    "        for j in range(0,numClass+1):\n",
    "            temp.append(0)\n",
    "        mat2.append(temp)\n",
    "    for i in range(1,numClass+1):\n",
    "        mat1[1][i] = 1\n",
    "        mat2[1][i] = 0\n",
    "        for j in range(2,len(dataList)+1):\n",
    "            mat2[j][i] = float('inf')\n",
    "    v = 0.0\n",
    "    for l in range(2,len(dataList)+1):\n",
    "        s1 = 0.0\n",
    "        s2 = 0.0\n",
    "        w = 0.0\n",
    "        for m in range(1,l+1):\n",
    "            i3 = l - m + 1\n",
    "            val = float(dataList[i3-1])\n",
    "            s2 += val * val\n",
    "            s1 += val\n",
    "            w += 1\n",
    "            v = s2 - (s1 * s1) / w\n",
    "            i4 = i3 - 1\n",
    "            if i4 != 0:\n",
    "                for j in range(2,numClass+1):\n",
    "                    if mat2[l][j] >= (v + mat2[i4][j - 1]):\n",
    "                        mat1[l][j] = i3\n",
    "                        mat2[l][j] = v + mat2[i4][j - 1]\n",
    "        mat1[l][1] = 1\n",
    "        mat2[l][1] = v\n",
    "    k = len(dataList)\n",
    "\n",
    "    breaks = []\n",
    "    for i in range(0,numClass+1):\n",
    "        breaks.append(min(dataList))\n",
    "    countNum = numClass\n",
    "\n",
    "    while countNum >= 2:#print \"rank = \" + str(mat1[k][countNum])\n",
    "        id = int((mat1[k][countNum]) - 2)\n",
    "        value = dataList[id]\n",
    "        breaks.append(value)\n",
    "        k = int((mat1[k][countNum] - 1))\n",
    "        countNum -= 1\n",
    "\n",
    "    breaks.append(max(dataList))\n",
    "    breaks = list(set(breaks))\n",
    "    breaks.sort()\n",
    "    return(breaks)\n",
    "\n",
    "    # The first output number is the smallest value in the input\n",
    "    # Following output numbers are inclusive upper bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are used to determine the goodness of variance fit, determining how well the function above\n",
    "# performed (in other words, it measures how well using a specific number of breakpoints applies to the data)\n",
    "\n",
    "def goodness_of_variance_fit(array, nclasses):\n",
    "    \n",
    "        classes = getJenksBreaks(array, nclasses)\n",
    "        classified = np.array([classify(i, classes) for i in array])\n",
    "        maxz = max(classified)\n",
    "        zone_indices = [[idx for idx, val in enumerate(classified) if zone + 1 == val] for zone in range(maxz)]\n",
    "        sdam = np.sum((array - array.mean()) ** 2)\n",
    "        array_sort = [np.array([array[index] for index in zone]) for zone in zone_indices]\n",
    "        sdcm = sum([np.sum((classified - classified.mean()) ** 2) for classified in array_sort])\n",
    "        gvf = (sdam - sdcm) / sdam\n",
    "    \n",
    "        return gvf, classes\n",
    "    \n",
    "def classify(value, breaks):\n",
    "    for i in range(1, len(breaks)):\n",
    "        if value < breaks[i]:\n",
    "            return i\n",
    "    return len(breaks) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function iterates over various numbers of breakpoints and uses the function above to find the best \n",
    "# number of break points\n",
    "\n",
    "def optimal_classes(data, gvf_threshold, min_classes, max_classes):\n",
    "\n",
    "    gvf= 0.0\n",
    "    nclasses = min_classes\n",
    "\n",
    "    while gvf < gvf_threshold:\n",
    "            gvf, classes = goodness_of_variance_fit(data, nclasses)\n",
    "            if gvf < gvf_threshold:\n",
    "                gvf, classes = goodness_of_variance_fit(data, nclasses)\n",
    "                print('Classes Trialed: ', nclasses)\n",
    "                print('gvf value :', gvf)\n",
    "                print('Outcome: gvf value insufficient, further trials will be attempted')\n",
    "                print()\n",
    "                nclasses += 1\n",
    "            else:\n",
    "                print('Classes Trialed: ', nclasses)\n",
    "                print('gvf value: ', gvf)\n",
    "                print('Breaks: ', classes)\n",
    "                print('Outcome: Ladies and gentlemen, we got him.')\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to determine the natural breaks of the Area_Slide variable. The first class will be records with an area of 0 (no landslide). Of the remaining records, natural breaks in the data will be determined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df of non-zero Area_Slide\n",
    "filtered_data = data[data['Area_Slide'] > 0]\n",
    "non_zero_slide_area = filtered_data['Area_Slide']\n",
    "non_zero_slide_area = np.array(non_zero_slide_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal breaks in non-zero Area_Slide\n",
    "optimal_classes(non_zero_slide_area, 0.85, 2, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these breaks to create a new column in the main dataframe applying a class to each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data[\"Area_Slide\"] > 0) & (data[\"Area_Slide\"] < 0.054691), \"Slide_Class\"] = \"Class 0\" \n",
    "data.loc[(data[\"Area_Slide\"] >= 0.054691) & (data[\"Area_Slide\"] <= 2981.98645), \"Slide_Class\"] = \"Class 1\" \n",
    "data.loc[(data[\"Area_Slide\"] > 2981.98645) & (data[\"Area_Slide\"] <= 10821.34832), \"Slide_Class\"] = \"Class 2\" \n",
    "data.loc[(data[\"Area_Slide\"] > 10821.34832) & (data[\"Area_Slide\"] <= 29509.81466), \"Slide_Class\"] = \"Class 3\" \n",
    "data.loc[(data[\"Area_Slide\"] > 29509.81466) & (data[\"Area_Slide\"] <= 82850.15392), \"Slide_Class\"] = \"Class 4\"\n",
    "data.loc[(data[\"Area_Slide\"] > 82850.15392) & (data[\"Area_Slide\"] <= 182966.8526), \"Slide_Class\"] = \"Class 5\"\n",
    "data[\"Slide_Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the value_counts of each class\n",
    "data[\"Slide_Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two major observations here:\n",
    "1) There is a large class imbalance, this will negatively affect training data. Oversampling will take place later to fix this problem.\n",
    "\n",
    "\n",
    "2) Oversampling requires a number of similar neighbors, so classes with inadequate value counts will be merged to facilitate oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data[\"Area_Slide\"] > 0) & (data[\"Area_Slide\"] < 0.054691), \"Slide_Class\"] = \"Class 0\" \n",
    "data.loc[(data[\"Area_Slide\"] >= 0.054691) & (data[\"Area_Slide\"] <= 2981.98645), \"Slide_Class\"] = \"Class 1\" \n",
    "data.loc[(data[\"Area_Slide\"] > 2981.98645) & (data[\"Area_Slide\"] <= 10821.34832), \"Slide_Class\"] = \"Class 1\" \n",
    "data.loc[(data[\"Area_Slide\"] > 10821.34832) & (data[\"Area_Slide\"] <= 29509.81466), \"Slide_Class\"] = \"Class 2\" \n",
    "data.loc[(data[\"Area_Slide\"] > 29509.81466) & (data[\"Area_Slide\"] <= 82850.15392), \"Slide_Class\"] = \"Class 3\"\n",
    "data.loc[(data[\"Area_Slide\"] > 82850.15392) & (data[\"Area_Slide\"] <= 182966.8526), \"Slide_Class\"] = \"Class 3\"\n",
    "data[\"Slide_Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dependant and explanatory variables\n",
    "\n",
    "data_y = data['Slide_Class']\n",
    "data_x = data.drop(['SU_ID', 'Count', 'Area_Slide', 'Slide_Class'], axis=1)\n",
    "data_x = data_x.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide into training and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train_area, y_test_area = train_test_split(data_x, data_y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the explanatory variables\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "train_x_values = x_train.values\n",
    "x_scaled1 = scaler.fit_transform(train_x_values)\n",
    "x_train = pd.DataFrame(x_scaled1)\n",
    "\n",
    "test_x_values = x_test.values\n",
    "x_scaled2 = scaler.fit_transform(test_x_values)\n",
    "x_test = pd.DataFrame(x_scaled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SMOTE package to oversample training data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "x_train, y_train_area = oversample.fit_resample(x_train, y_train_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Slide_Class field to create a binary variable that \n",
    "# indicates whether or not there is a landslide in any capacity\n",
    "\n",
    "y_train_count = []\n",
    "y_test_count = []\n",
    "\n",
    "for i in y_train_area:\n",
    "    if i == \"Class 0\":\n",
    "        y_train_count.append(0)\n",
    "    else:\n",
    "        y_train_count.append(1)\n",
    "        \n",
    "for i in y_test_area:\n",
    "    if i == \"Class 0\":\n",
    "        y_test_count.append(0)\n",
    "    else:\n",
    "        y_test_count.append(1)\n",
    "        \n",
    "y_train_count = np.array(y_train_count)\n",
    "y_test_count = np.array(y_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of preprocessing is to encode the area classes for input into the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "#training\n",
    "encoder.fit(y_train_area)\n",
    "encoded_Y = encoder.transform(y_train_area)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "y_train = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "#testing\n",
    "encoder.fit(y_test_area)\n",
    "encoded_Y_test = encoder.transform(y_test_area)\n",
    "y_test = np_utils.to_categorical(encoded_Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Modelling Process</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.initializers import RandomNormal\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE MONTE CARLO TUNING\n",
    "### This process takes hours, so if you are interested in running this code, please \n",
    "### use the placeholder values listed below:\n",
    "\n",
    "### placeholder = (6,6,28,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_muti_output_model(6, 6, 28, 4)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-2)\n",
    "model.compile(loss=[\"binary_crossentropy\", \"categorical_crossentropy\"], \n",
    "              optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, [y_train_count, y_train], \n",
    "                    epochs=300,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(patience=15)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training Evaluation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training: Count Output</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss during training\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplot()\n",
    "plt.title('Count loss ')\n",
    "plt.plot(history.history['count_output_loss'], label='loss')\n",
    "plt.plot(history.history['val_count_output_loss'], label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss during training\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplot()\n",
    "plt.title('Area loss ')\n",
    "plt.plot(history.history['class_output_loss'], label='loss')\n",
    "plt.plot(history.history['val_class_output_loss'], label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training: Area Output</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluation Using Test Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "count_predictions = y_pred[0]\n",
    "area_predictions = y_pred[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, when evaluating classifiers on imbalanced data, ROC AUC is not the preffered evaluation metric. Instead, a precision and recal curve would be used. In this case, this model was designed to outperform a statistical approach to the same problem (not shown in this notebook). The paper in question used ROC AUC to evaluate its perfromance (perhaps not the ideal measure) but in light of this, ROC AUC is also used here to enable direct comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test: Count Output</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bcdb7704c100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# calculate scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlr_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# summarize scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NN: ROC AUC=%.6f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlr_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test_count' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# calculate scores\n",
    "count_auc = roc_auc_score(y_test_count, count_predictions)\n",
    "# summarize scores\n",
    "print('NN: ROC AUC=%.6f' % (count_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_probs = [0 for _ in range(len(y_test_count))]\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test_count, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test_count, count_predictions)\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test_count, count_predictions)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(recall, precision, label='Logistic Regression')\n",
    "baseline = len(y_test_count[y_test_count==1]) / len(y_test_count)\n",
    "ax.plot([0, 1], [baseline, baseline], linestyle='--', label='Baseline')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.legend(loc='center left');\n",
    "print(\"Mean Precision: \",np.mean(precision))\n",
    "print(\"Mean Recall:    \", np.mean(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test: Area Output</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_roc_auc_ovo = roc_auc_score(y_test_area, area_predictions, multi_class=\"ovo\",\n",
    "                                  average=\"macro\")\n",
    "weighted_roc_auc_ovo = roc_auc_score(y_test_area, area_predictions, multi_class=\"ovo\",\n",
    "                                     average=\"weighted\")\n",
    "macro_roc_auc_ovr = roc_auc_score(y_test_area, area_predictions, multi_class=\"ovr\",\n",
    "                                  average=\"macro\")\n",
    "weighted_roc_auc_ovr = roc_auc_score(y_test_area, area_predictions, multi_class=\"ovr\",\n",
    "                                     average=\"weighted\")\n",
    "print(\"One-vs-One ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
    "      \"(weighted by prevalence)\"\n",
    "      .format(macro_roc_auc_ovo, weighted_roc_auc_ovo))\n",
    "print(\"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
    "      \"(weighted by prevalence)\"\n",
    "      .format(macro_roc_auc_ovr, weighted_roc_auc_ovr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "def plot_multiclass_roc(y_test, y_pred, n_classes, figsize=(17, 6)):\n",
    "\n",
    "    # structures\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    # calculate dummies once\n",
    "    y_test_dummies = pd.get_dummies(y_test, drop_first=False).values\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_dummies[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # roc for each class\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curves for Each Class')\n",
    "    for i in range(n_classes):\n",
    "        ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.6f) for label %i' % (roc_auc[i], i))\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.grid(alpha=.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiclass_roc(y_test_area, area_predictions, n_classes=4, figsize=(12, 9))\n",
    "# (4, 6, 16, 6)\n",
    "# 0.87, 0.81, 0.88, 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_muti_output_model(6, 6, 28, 4)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-2)\n",
    "model.compile(loss=[\"binary_crossentropy\", \"categorical_crossentropy\"], \n",
    "              optimizer='adam',\n",
    "              loss_weights={\"count_output\": 0.6, \"class_output\": 0.4})\n",
    "history = model.fit(x_train, [y_train_count, y_train], \n",
    "                    epochs=300,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(patience=15)])\n",
    "y_pred=model.predict(x_test)\n",
    "count_predictions = y_pred[0]\n",
    "area_predictions = y_pred[1]\n",
    "# 6,9,28,4 produced 0.901\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# calculate scores\n",
    "lr_auc = roc_auc_score(y_test_count, count_predictions)\n",
    "# summarize scores\n",
    "print('NN: ROC AUC=%.6f' % (lr_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lamda_testing():\n",
    "    weight_1 = 0\n",
    "    weight_2 = 1-weight_1\n",
    "    runs = 5\n",
    "    while weight_1 <= 1:\n",
    "        model = build_muti_output_model(6, 6, 28, 4)\n",
    "        model.compile(loss=[\"binary_crossentropy\", \"categorical_crossentropy\"], \n",
    "              optimizer='adam',\n",
    "              loss_weights={\"count_output\": weight_1, \"class_output\": weight_2})\n",
    "        for i in range(runs):\n",
    "            history = model.fit(x_train, [y_train_count, y_train], \n",
    "                    epochs=300,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(patience=15)],\n",
    "                    verbose=0)\n",
    "            y_pred=model.predict(x_test)\n",
    "            count_predictions = y_pred[0]\n",
    "            area_predictions = y_pred[1]\n",
    "            lr_auc = roc_auc_score(y_test_count, count_predictions)\n",
    "            # summarize scores\n",
    "            print(weight_1, weight_2)\n",
    "            print('NN: ROC AUC=%.6f' % (lr_auc))\n",
    "        weight_1 += 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
